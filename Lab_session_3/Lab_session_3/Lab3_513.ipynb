{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# AERO 5 - Hands on Machine Learning for cybersecurity (2023/2024)\n",
    "\n",
    "\n",
    "# 3 – Malicious URLs detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is done by (write the members of the group below):\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session we will discuss how the Machine Learning is used for the malicious URLs detection. This will first involve in cleaning of our data within the datasets. We will use pandas and define our own vectorizer to clear the datasets. More on this later, we will use Logistic Regression then the Support Vector Machine to train our model! The `scikit-learn` documentation is complete and should be consulted whenever necessary. In particular you can consult :\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a binary classification technique. A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value. In this exercise, we will apply a logistic regression model to a preprocessed dataset resulting from phishing websites `phishing_dataset.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by importing the necessary packages that allows you to create matrices, perform mathematical operations and create graphs to easily observe our dataset as well as the model built from it. Then load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"phishing_dataset.csv\", header=None)\n",
    "df\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Could you propose an implementation of the logistic regression method without using the `scikit-learn` package?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "# Auteur : Maxime Gosselin\n",
    "# Titre  : Régression Logistique\n",
    "# Date   : Novembre 2023\n",
    "# Lien   : https://github.com/bixente-r/ML_project\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize,fmin_tnc\n",
    "import seaborn as sn\n",
    "from scipy.interpolate import BSpline\n",
    "from scipy.signal import savgol_filter\n",
    "import statistics as st\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def convert_X(X):\n",
    "    for i in X[0:0]:\n",
    "        dx = {}\n",
    "        k = 0\n",
    "        if isinstance((X[i][0]), str) == True:\n",
    "            for j in range(X.shape[0]):\n",
    "                if X[i][j] not in dx.keys():\n",
    "                    dx[X[i][j]] = k\n",
    "                    k += 1\n",
    "            for l in range(X.shape[0]):\n",
    "                for j in dx.keys():\n",
    "                    if X[i][l] == j:\n",
    "                        X[i][l] = dx[j]\n",
    "\n",
    "\n",
    "def convert_y(y):\n",
    "    dy = {}\n",
    "    k = 0\n",
    "\n",
    "    if isinstance((y[0]), str) == True:\n",
    "        for i in range(0,y.shape[0]):\n",
    "            if y[i] not in dy.keys():\n",
    "                dy[y[i]] = k\n",
    "                k += 1\n",
    "\n",
    "        for i in range(0,y.shape[0]):\n",
    "            for j in dy.keys():\n",
    "                if y[i] == j:\n",
    "                    y[i] = dy[j]\n",
    "\n",
    "def init_frame(path, id_col=0):\n",
    "\n",
    "    df = pd.read_csv(path, header=None) # import the data\n",
    "    df.head()\n",
    "    df = df.dropna().reset_index(drop=True) # remove rows with a Nan value from dataset and reset the index\n",
    "    \n",
    "    target_class = df.columns[-1]\n",
    "\n",
    "    nb_row = df.shape[0]\n",
    "    print(nb_row)\n",
    "    \n",
    "    # print(df)\n",
    "    #df_0 = df.loc[df[target_class] == -1].sample(n=nb_row,random_state=42)\n",
    "    #df_1 = df.loc[df[target_class] == 1]\n",
    "    #df = pd.concat([df_0, df_1])\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True) # shuffle the rows \n",
    "\n",
    "    X = df.iloc[:,id_col:-1] # Separate input from output \n",
    "    y = df.iloc[:,-1]  # Check rows and columns from .csv\n",
    "                        # parameters might change\n",
    "\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    convert_X(X)\n",
    "    convert_y(y)\n",
    "    # print(X)\n",
    "    # print(y)\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X] # add a bias column\n",
    "    X = np.array([[float(i) for i in e] for e in X]) # convert to float (in case it's not)\n",
    "    row_nb = X.shape[0]\n",
    "    traning_nb = round(0.8*row_nb)\n",
    "    X1 = X[:traning_nb] # training set\n",
    "    X2 = X[traning_nb:] # validation set\n",
    "\n",
    "\n",
    "    y = y.to_numpy() # convert to numpy type\n",
    "    y = y.reshape(len(y),1) # convert to matrix\n",
    "    y = np.array([[float(i) for i in e] for e in y]) # convert to float (in case it's not)\n",
    "    y1 = y[:traning_nb] # training output\n",
    "    y2 = y[traning_nb:] # validation output\n",
    "    return X1,y1, X2,y2\n",
    "\n",
    "def sigmoid(x, theta):\n",
    "    z = np.dot(x, theta)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def hypothesis(theta, x):\n",
    "    return sigmoid(x, theta)\n",
    "\n",
    "\n",
    "def cost_function(theta, x, y, n):\n",
    "    h = hypothesis(theta, x)\n",
    "    return -(1/n)*np.sum(y*np.log(h) + (1-y)*np.log(1-h))\n",
    "\n",
    "\n",
    "def gradient(theta, X, y, n):\n",
    "    h = hypothesis(theta, X)\n",
    "    error = h - y\n",
    "    return (1/n) * np.dot(X.T, error)\n",
    "\n",
    "def predict(h):\n",
    "    \n",
    "    h1 = []\n",
    "    for i in h:\n",
    "        if i>=0.5:\n",
    "            h1.append(1)\n",
    "        else:\n",
    "            h1.append(0)\n",
    "    return h1\n",
    "\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    return round(100 * (TP + TN) / (TP + TN + FP + FN),4)\n",
    "\n",
    "def precision_1(TP,FP):\n",
    "    s = 0\n",
    "    try:\n",
    "        s = round(100 * TP / (TP + FP),4)\n",
    "    except:\n",
    "        ZeroDivisionError()\n",
    "    return s\n",
    "\n",
    "def precision_0(TN,FN):\n",
    "    return round(100 * TN / (TN + FN),4)\n",
    "\n",
    "def recall(TP,FN):\n",
    "    return round(100 * TP / (TP + FN),4)\n",
    "\n",
    "def specificity(TN,FP):\n",
    "    return round(100 * TN / (TN + FP),4)\n",
    "\n",
    "def f1_score(precision,recall):\n",
    "    s = 0\n",
    "    try:\n",
    "        s = round((1/100) * 2 * (precision * recall) / (precision + recall),4)\n",
    "    except:\n",
    "        ZeroDivisionError()\n",
    "    return s\n",
    "\n",
    "def confusion_matrix(TP,FP,TN,FN):\n",
    "\n",
    "    col = [\"Positive\", \"Negative\"]\n",
    "    ind = [\"Positive\", \"Negative\"]\n",
    "\n",
    "    matrix=np.array([[TP,FN],[FP,TN]])\n",
    "    df = pd.DataFrame(matrix,columns=col,index=ind)\n",
    "    fig = sn.heatmap(df,annot=True,cbar=True,fmt='g',cmap=\"flare\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"Actual Class\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(Xt, yt, Xv, yv, nt, nv, epoch, alpha, graph=True, disp=True):\n",
    "    \"\"\"\n",
    "    Function that train the logistic regression model\n",
    "\n",
    "    PARAMETERS : \n",
    "\n",
    "        - Xt : Training set            ¤     - Xv : Validation set\n",
    "        - yt : Training set targets    ¤     - yv : Validation set targets\n",
    "        - nt : number of training set  ¤     - nv : number of validation set\n",
    "        - epoch                        ¤     - alpha : param of gradient descent\n",
    "        - graph : display the graphs\n",
    "    \n",
    "    OUTPUT : \n",
    "\n",
    "        theta : vector of the optimal weights\n",
    "\n",
    "    PROCESS : \n",
    "\n",
    "        for each epoch :\n",
    "            we compute the new weigths (theta) from the training set \n",
    "            we compute the loss for the training set\n",
    "            we compute the accuracy for the training set \n",
    "            we use the weights from training set to compute loss and accuracy of the validation set\n",
    "        \n",
    "        we return the final weights for the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################\n",
    "    #####                SETTING VARIABLES                #####\n",
    "    ###########################################################\n",
    "    training_err = []\n",
    "    validation_err = []\n",
    "\n",
    "    training_acc = []\n",
    "    training_pre_0 = []\n",
    "    training_pre_1 = []\n",
    "    training_rec = []\n",
    "    training_spe = []\n",
    "    training_f1 = []\n",
    "\n",
    "    validation_acc = []\n",
    "    validation_pre_0 = []\n",
    "    validation_pre_1 = []\n",
    "    validation_rec = []\n",
    "    validation_spe = []\n",
    "    validation_f1 = []\n",
    "\n",
    "    last_t_loss = None\n",
    "    theta = np.zeros((Xt.shape[1], 1))\n",
    "  \n",
    "\n",
    "    ###########################################################\n",
    "    #####                 BEGIN TRAINING                  #####\n",
    "    ###########################################################\n",
    "    for e in tqdm(range(1, epoch + 1)):\n",
    "\n",
    "        grad = gradient(theta,Xt,yt, nt)   # gradient computation\n",
    "        theta = theta - alpha * grad * cost_function(theta, Xt, yt, nt)   # updating weights\n",
    "        \n",
    "        loss_t = cost_function(theta, Xt, yt, nt)  # compute loss with the current weight (training set)\n",
    "        training_err.append(loss_t)\n",
    "\n",
    "        out_t = hypothesis(theta, Xt)    # sigmoid application with the current weight (training set)\n",
    "        yt_pred = predict(out_t)          # prediction h > 0.5 or h < 0.5\n",
    "        \n",
    "        TP, TN, FN, FP = 0,0,0,0\n",
    "\n",
    "        for i in range(nt):\n",
    "            \n",
    "            if yt_pred[i] == yt[i] and yt[i] == 1:\n",
    "                TP += 1\n",
    "            if yt_pred[i] != yt[i] and yt_pred[i] == 1:\n",
    "                FP += 1\n",
    "            if yt_pred[i] == yt[i] and yt[i] == 0:\n",
    "                TN += 1\n",
    "            if yt_pred[i] != yt[i] and yt_pred[i] == 0:\n",
    "                FN += 1\n",
    "        \n",
    "\n",
    "        acc_t = accuracy(TP, TN, FP, FN)   \n",
    "        pre_0_t = precision_0(TN, FN)\n",
    "        pre_1_t = precision_1(TP, FP)\n",
    "        rec_t = recall(TP, FN)\n",
    "        spe_t = specificity(TN, FP)\n",
    "        f1_t = f1_score(pre_1_t, rec_t)\n",
    "\n",
    "        training_acc.append(acc_t)\n",
    "        training_pre_0.append(pre_0_t)\n",
    "        training_pre_1.append(pre_1_t)\n",
    "        training_rec.append(rec_t)\n",
    "        training_spe.append(spe_t)\n",
    "        training_f1.append(f1_t)\n",
    "        \n",
    "        if e == epoch:\n",
    "            a,b,c,d = TP, FP, TN, FN\n",
    "\n",
    "        loss_v = cost_function(theta, Xv, yv, nv)  # compute loss with the current weight (validation set)\n",
    "        validation_err.append(loss_v)\n",
    "        \n",
    "        \n",
    "        out_v = hypothesis(theta,Xv)  # sigmoid application with the current weight (validation set)\n",
    "        yv_pred = predict(out_v)     # prediction h > 0.5 or h < 0.5\n",
    "       \n",
    "        TP, TN, FN, FP = 0,0,0,0\n",
    "\n",
    "        for i in range(nv): \n",
    "            if yv_pred[i] == yv[i] and yv[i] == 1:\n",
    "                TP += 1\n",
    "            if yv_pred[i] != yv[i] and yv_pred[i] == 1:\n",
    "                FP += 1\n",
    "            if yv_pred[i] == yv[i] and yv[i] == 0:\n",
    "                TN += 1\n",
    "            if yv_pred[i] != yv[i] and yv_pred[i] == 0:\n",
    "                FN += 1\n",
    "        \n",
    "        acc_v = accuracy(TP, TN, FP, FN)   \n",
    "        pre_0_v = precision_0(TN, FN)\n",
    "        pre_1_v = precision_1(TP, FP)\n",
    "        rec_v = recall(TP, FN)\n",
    "        spe_v = specificity(TN, FP)\n",
    "        f1_v = f1_score(pre_1_v, rec_v)\n",
    "\n",
    "        validation_acc.append(acc_v)\n",
    "        validation_pre_0.append(pre_0_v)\n",
    "        validation_pre_1.append(pre_1_v)\n",
    "        validation_rec.append(rec_v)\n",
    "        validation_spe.append(spe_v)\n",
    "        validation_f1.append(f1_v)        \n",
    "        \n",
    "\n",
    "        if (e % (epoch / 10) == 0 or e == epoch) and disp == True:\n",
    "            print(f'\\n¤¤¤¤¤¤¤¤¤¤¤¤ EPOCH {e} ¤¤¤¤¤¤¤¤¤¤¤¤')\n",
    "            if last_t_loss and last_t_loss < loss_t:\n",
    "                print('   >>>>> LOSS INCREASING <<<<<   ')\n",
    "            else:\n",
    "                print(f' - Training loss : {round(loss_t,4)}')\n",
    "                print(f' - Validation loss : {round(loss_v,4)}')\n",
    "                print(f' - Loss difference : {round(((loss_t - loss_v)/loss_t),4)} % ')\n",
    "                print(f' - Training accuracy : {training_acc[-1]} %')\n",
    "                print(f' - Validation accuracy : {validation_acc[-1]} %')\n",
    "                print(f' - Training 1 precision : {training_pre_1[-1]} %')\n",
    "                print(f' - Validation 1 precision : {validation_pre_1[-1]} %')\n",
    "                print(f' - Training 0 precision : {training_pre_0[-1]} %')\n",
    "                print(f' - Validation 0 precision : {validation_pre_0[-1]} %')\n",
    "                print(f' - Training recall : {training_rec[-1]} %')\n",
    "                print(f' - Validation recall : {validation_rec[-1]} %')\n",
    "                print(f' - Training specificity : {training_spe[-1]} %')\n",
    "                print(f' - Validation specificity : {validation_spe[-1]} %')\n",
    "                print(f' - Training f1-score : {training_f1[-1]}')\n",
    "                print(f' - Validation f1-score : {validation_f1[-1]}')\n",
    "                print(f'¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤¤')\n",
    "            \n",
    "            last_t_loss = loss_t\n",
    "        \n",
    "    confusion_matrix(a, b, c, d)\n",
    "\n",
    "\n",
    "    if disp == True:\n",
    "        print(f'\\n¤¤¤¤¤¤¤¤¤¤¤¤  OPTIMAL THETA ¤¤¤¤¤¤¤¤¤¤¤¤\\n')\n",
    "        for i in range(theta.shape[0]):\n",
    "            print(f'w{i} = {theta[i][0]}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if graph == True:\n",
    "        ###########################################################\n",
    "        #####                    PLOT LOSS                    #####\n",
    "        ###########################################################\n",
    "        plt.figure()\n",
    "        plt.title('Loss')\n",
    "        plt.plot(training_err, 'dodgerblue', label='Training loss')\n",
    "        plt.plot(validation_err, 'darkorange', label='Validation loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        ###########################################################\n",
    "        #####                  PLOT ACCURACY                  #####\n",
    "        ###########################################################\n",
    "        plt.figure()\n",
    "        plt.title('Accuracy')\n",
    "        plt.plot(training_acc, 'dodgerblue', label='Training accuracy')\n",
    "        plt.plot(validation_acc, 'darkorange', label='Validation accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('f1-score')\n",
    "        plt.plot(training_f1, 'dodgerblue', label='Training f1-score')\n",
    "        plt.plot(validation_f1, 'darkorange', label='Validation f1-score')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('f1-score')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Recall')\n",
    "        plt.plot(training_rec, 'dodgerblue', label='Training recall')\n",
    "        plt.plot(validation_rec, 'darkorange', label='Validation recall')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('recall')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return theta, training_err, validation_err\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load your dataset, analyse it then put the attributes and the targets in variables \"samples\" and \"targets\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "df = pd.read_csv('phishing_dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Extract features (attributes) - excluding the last column\n",
    "samples = df.iloc[:, :-1]\n",
    "\n",
    "# Extract the target variable - the last column\n",
    "target_column_name = df.columns[-1]\n",
    "targets = df[target_column_name]\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vérifier si la série contient des zéros\n",
    "if 0 in targets.values:\n",
    "    print(\"Il y a des zéros dans la série.\")\n",
    "else:\n",
    "    print(\"Il n'y a pas de zéros dans la série.\")\n",
    "\n",
    "# Si la série contient uniquement des 1 et -1, imprimer 1 et -1\n",
    "if set(targets.unique()) == {1, -1}:\n",
    "    print(\"La série contient uniquement des 1 et -1.\")\n",
    "\n",
    "\n",
    "# Remplacer tous les -1 par 0\n",
    "targets.replace(-1, 0, inplace=True)\n",
    "\n",
    "\n",
    "plt.hist(targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "training_set, training_output, validation_set, validation_output = init_frame(\"./phishing_dataset.csv\") \n",
    "\n",
    "training_output[training_output == -1] = 0\n",
    "validation_output[validation_output == -1] = 0\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Apply now the logistic regression to train and test the model then return the mean accuracy on the given test data by computing fist the model score then the error matrix: the confusion matrix. Comment on the obtained resut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "n_training = training_set.shape[0]\n",
    "n_validation = validation_set.shape[0]\n",
    "\n",
    "alpha = 0.001\n",
    "epoch = 5000\n",
    "w = train(training_set,training_output,validation_set, validation_output, n_training, n_validation, epoch, alpha,True,True)\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = training_set.shape[0]\n",
    "n_validation = validation_set.shape[0]\n",
    "\n",
    "alpha = 0.01\n",
    "epoch = 2000\n",
    "w = train(training_set,training_output,validation_set, validation_output, n_training, n_validation, epoch, alpha,True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the logistic regression has good results but we could try to test different model because the loss remain a bit high for a binary classification. However more iteration in the training would lead to overfitting as we notice the two loss diverging at the end of the iteration (see loss graph). \n",
    "\n",
    "Moreover, it is important to have the less False Positive and False Negative predictions. It could be annoying to have false alert or dangerous to miss alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Data collection:\n",
    "\n",
    "   The first task is gathering data. We can find some websites offering malicious links while browsing. The second task is finding out clear URLs. This time, we use a data set that is already available, and that doesn’t need to be crawled. Hence, we gathered around 500,000 URLs out of which around 90,000 were malicious and others were legitimate/clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Preparing data:\n",
    "\n",
    "   Since the URLs are different from our normal text documents, they need to undergo some amount of cleasing before we use them. Therefore, we tokenize them by removing slash, dots and coms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import `numpy`, `pandas` and `random` librairies. Then Write a sanitization function to get the relevant data from raw URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "def url_cleanse(web_url)\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us the desired url data-set values to train the model and test it. The dataset will have two column structures; one for urls and one for labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the the datasets into dataframes and matrix which can be understood by the vectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv = pd.read_csv('data_url.csv',',',error_bad_lines=False)\n",
    "url_df = pd.DataFrame(url_csv)              # to convert into data frames                                                                                  \n",
    "url_df = np.array(url_df)                   # to convert into array   \n",
    "random.shuffle(url_df)\n",
    "y = [d[1] for d in url_df]                  # all labels\n",
    "urls = [d[0] for d in url_df]               # all urls corresponding to a label {G/B}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, data can be understood by the vectorizer we prepared and later pass onto the term-frequency and inverse document frequency text extraction approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use `TF-IDF` Machine Learning text feature extraction approach from the `scikit-learn` python module in order to pass the data to a custom vectorizer function. For more understanding consult \n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split data into training and testing sets then use the logistic regression from `scikit-learn` python module to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Give the mean accuracy on the given test data by computing the score. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use the model to test on URLs ['hackthebox.eu,'google.com/search=VAD3R','wikipedia.co.uk'].\n",
    "                               What do you remark? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the SVM approach to detect the malicious URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the required librairies available in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the SVM classifier from `scikit-learn` python module to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Once the model is trained with the SVM classifier, load the model and the vector to predict the URL : ['google.com/search=VAD3R']. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the previous datasets and apply a classification method of your choice to predict malicious URLs. Describe the different steps and the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT THIS CELL\n",
    "# ====================== Your code here =================\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
